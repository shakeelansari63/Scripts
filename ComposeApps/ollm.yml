name: freak-ollama
networks:
  default:
    name: freak-ollama-net
services:
  # Add extra commands which can be run on this service through compose app runner
  ollama:
    annotations:
      extracmd-pull: ollama pull {##}
      extracmddesc-pull: Pull a model from Ollama registry
      extracmd-list: ollama list
      extracmddesc-list: List available models
      extracmd-ps: ollama ps
      extracmddesc-ps: List running models
      extracmd-kill: ollama stop {##}
      extracmddesc-kill: Kill a running model
      extracmd-run: ollama run {##}
      extracmddesc-run: Run and Model and start chat
    image: docker.io/ollama/ollama:latest
    container_name: freak-ollama-ollama
    environment:
      OLLAMA_DEBUG: "1"
    healthcheck:
      test: ["CMD-SHELL", "/bin/bash -c ':> /dev/tcp/127.0.0.1/11434'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    ports:
      - "11434:11434"
    volumes:
      - ${DEV_MOUNT_POINT}/System/ContainerVolumes/OLLAMA:/root/.ollama

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: freak-open-webui
    volumes:
      - ${DEV_MOUNT_POINT}/System/ContainerVolumes/OPENWEBUI:/app/backend/data
    ports:
      - 53081:8080
    healthcheck:
      test: ["CMD-SHELL", "/bin/bash -c ':> /dev/tcp/127.0.0.1/8080'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      OLLAMA_BASE_URL: "http://ollama:11434"
      WEBUI_SECRET_KEY: ""
      ENABLE_SIGNUP: "False"
      PORT: 8080
      WEBUI_AUTH: "False"
    restart: always
    depends_on:
      ollama:
        condition: service_healthy
